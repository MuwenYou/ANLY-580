local_slide( {"name":"B8642EDC-B7B7-4E9A-AE01-5B05C5F811BB","json":{"assets":{"29C21EB09B15C8AA1385CC13E9F1C782":{"type":"texture","index":0,"assetRequest":{"type":"slide","state":"contents","slide":"none"},"url":{"native":"assets\/B8642EDC-B7B7-4E9A-AE01-5B05C5F811BB.pdf"},"width":1920,"height":1080},"23B81D527B6FE1A3546B4B51A3907413":{"type":"texture","index":1,"assetRequest":{"type":"slide","state":"contents","slide":"none"},"url":{"native":"assets\/B8642EDC-B7B7-4E9A-AE01-5B05C5F811BB.pdf"},"width":1920,"height":1080}},"events":[{"effects":[{"beginTime":0,"baseLayer":{"animations":[],"initialState":{"affineTransform":[1,0,0,1,0,0],"masksToBounds":false,"rotation":0,"scale":1,"position":{"pointX":960,"pointY":540},"width":1920,"sublayerTransform":[1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1],"contentsRect":{"y":0,"x":0,"width":1,"height":1},"opacity":1,"edgeAntialiasingMask":251658240,"height":1080,"hidden":false,"anchorPoint":{"pointX":0.5,"pointY":0.5}},"objectID":"0","layers":[{"animations":[],"initialState":{"affineTransform":[1,0,0,1,0,0],"masksToBounds":false,"rotation":0,"scale":1,"position":{"pointX":960,"pointY":540},"width":1920,"sublayerTransform":[1,0,0,0,0,1,0,0,0,0,1,-0.00035007912466775983,0,0,0,1],"contentsRect":{"y":0,"x":0,"width":1,"height":1},"opacity":1,"edgeAntialiasingMask":251658240,"height":1080,"hidden":false,"anchorPoint":{"pointX":0.5,"pointY":0.5}},"layers":[{"animations":[],"layers":[],"texturedRectangle":{"isBackgroundTexture":false,"singleTextureOpacity":1,"textureType":0,"textBaseline":0,"textXHeight":0,"isVerticalText":false},"initialState":{"affineTransform":[1,0,0,1,0,0],"masksToBounds":false,"rotation":0,"scale":1,"position":{"pointX":960,"pointY":540},"width":1920,"sublayerTransform":[1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1],"contentsRect":{"y":0,"x":0,"width":1,"height":1},"opacity":1,"edgeAntialiasingMask":0,"height":1080,"hidden":false,"anchorPoint":{"pointX":0.5,"pointY":0.5}},"texture":"23B81D527B6FE1A3546B4B51A3907413"},{"animations":[{"additive":false,"timeOffset":0,"beginTime":0,"from":{"scalar":false},"repeatCount":0,"fillMode":"both","duration":0.01,"autoreverses":false,"property":"hidden","to":{"scalar":true},"removedOnCompletion":false}],"layers":[],"texturedRectangle":{"isBackgroundTexture":false,"singleTextureOpacity":1,"textureType":0,"textBaseline":0,"textXHeight":0,"isVerticalText":false},"initialState":{"affineTransform":[1,0,0,1,0,0],"masksToBounds":false,"rotation":0,"scale":1,"position":{"pointX":960,"pointY":540},"width":1920,"sublayerTransform":[1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1],"contentsRect":{"y":0,"x":0,"width":1,"height":1},"opacity":1,"edgeAntialiasingMask":0,"height":1080,"hidden":false,"anchorPoint":{"pointX":0.5,"pointY":0.5}},"texture":"29C21EB09B15C8AA1385CC13E9F1C782"}]}]},"effects":[],"duration":0.01,"type":"transition","attributes":{"direction":0},"name":"none","objectID":"0"}],"automaticPlay":false,"hyperlinks":[],"accessibility":[{"text":"n-gram modeling has a fundamental limitation","targetRectangle":{"y":4,"x":40,"width":1459.1500000000003,"height":84}},{"text":"Examples from Eisenstein (2018)","targetRectangle":{"y":767.08993935585022,"x":459.19051790237427,"width":287.154,"height":23.819999635219574}},{"text":"￼","targetRectangle":{"y":1038.7800302505493,"x":1890.8349829391809,"width":20.394000000000005,"height":26}},{"text":"pasted-image.pdf","targetRectangle":{"y":804.903076171875,"x":515.6885986328125,"width":843.28948974609375,"height":45.28125}},{"text":"In lecture-03 we learned that word2vec was motivated by this idea of capturing the meaning of words within their contexts. We didn’t formalize the learning problem as one of language modeling, but in fact it is a language model that uses the skip-gram assumptions (Markov assumption + context word ordering doesn’t matter). ","targetRectangle":{"y":132.66667175292969,"x":49.833276589710295,"width":1860.3499999999999,"height":244}},{"text":"But, are we really solving the problem? We’ve learned that n is practically limited to around 10 or so; even at n=10 we have a sparsity problem and are forced to introduce bias in order to even compute the probability of sequences. We can hardly say that we’re capturing context needed for understanding. For example, any reasonable model of human language should be able to capture the intended meaning of “crashed” in the following sentence:","targetRectangle":{"y":376.66667175292969,"x":49.833276589710295,"width":1874.3499999999999,"height":366}},{"text":"In practice, computing n-gram probabilities from occurrence frequency is adequate for some tasks, and far from adequate for others.","targetRectangle":{"y":925.66667175292969,"x":49.833276589710295,"width":1856.3500000000001,"height":122}}],"baseLayer":{"animations":[],"initialState":{"affineTransform":[1,0,0,1,0,0],"masksToBounds":false,"rotation":0,"scale":1,"position":{"pointX":960,"pointY":540},"width":1920,"sublayerTransform":[1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1],"contentsRect":{"y":0,"x":0,"width":1,"height":1},"opacity":1,"edgeAntialiasingMask":251658240,"height":1080,"hidden":false,"anchorPoint":{"pointX":0.5,"pointY":0.5}},"objectID":"0","layers":[{"animations":[],"initialState":{"affineTransform":[1,0,0,1,0,0],"masksToBounds":false,"rotation":0,"scale":1,"position":{"pointX":960,"pointY":540},"width":1920,"sublayerTransform":[1,0,0,0,0,1,0,0,0,0,1,-0.00035007912466775983,0,0,0,1],"contentsRect":{"y":0,"x":0,"width":1,"height":1},"opacity":1,"edgeAntialiasingMask":251658240,"height":1080,"hidden":false,"anchorPoint":{"pointX":0.5,"pointY":0.5}},"layers":[{"animations":[],"layers":[],"texturedRectangle":{"isBackgroundTexture":false,"singleTextureOpacity":1,"textureType":0,"textBaseline":0,"textXHeight":0,"isVerticalText":false},"initialState":{"affineTransform":[1,0,0,1,0,0],"masksToBounds":false,"rotation":0,"scale":1,"position":{"pointX":960,"pointY":540},"width":1920,"sublayerTransform":[1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1],"contentsRect":{"y":0,"x":0,"width":1,"height":1},"opacity":1,"edgeAntialiasingMask":0,"height":1080,"hidden":false,"anchorPoint":{"pointX":0.5,"pointY":0.5}},"texture":"29C21EB09B15C8AA1385CC13E9F1C782"}]}]}}]}} )
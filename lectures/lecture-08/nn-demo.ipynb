{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e40ef4c-0c8a-4ef4-b213-61a6f9bdce96",
   "metadata": {},
   "source": [
    "# Neural networks for text classification\n",
    "\n",
    "This demo provides very basic Numpy implementations of the following:\n",
    "\n",
    "1. A feed forward NN represented as a doubly linked list\n",
    "2. A text classifier that uses a CBOW feature representation\n",
    "3. Autograd\n",
    "\n",
    "*Note: This demo conveys some of the underlying constructs used in deep learning packages, not their implementations of those constructs.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529121b5-f844-452d-842e-0c881a7f0212",
   "metadata": {},
   "source": [
    "### Load DBPedia14 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0495d64a-b5ea-4807-b417-aa2d08e9d46f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chris/anaconda3/envs/ml-base/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Reusing dataset dbpedia_14 (/home/chris/.cache/huggingface/datasets/dbpedia_14/dbpedia_14/2.0.0/01dab9e10d969eadcdbc918be5a09c9190a24caeae33b10eee8f367a1e3f1f0c)\n",
      "100%|██████████| 2/2 [00:00<00:00, 501.59it/s]\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "M = 50000\n",
    "\n",
    "df = datasets.load_dataset(\n",
    "    'dbpedia_14', \n",
    "    split=['train[:100%]', \n",
    "           'test[100%:]']\n",
    ")[0].to_pandas().sample(frac=1).reset_index(drop=True)[:M]\n",
    "\n",
    "K = len(set(df.label))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c527b8",
   "metadata": {},
   "source": [
    "### GPU acceleration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e992753",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chris/anaconda3/envs/ml-base/lib/python3.9/site-packages/cupy/_environment.py:438: UserWarning: \n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "  CuPy may not function correctly because multiple CuPy packages are installed\n",
      "  in your environment:\n",
      "\n",
      "    cupy, cupy-cuda11x\n",
      "\n",
      "  Follow these steps to resolve this issue:\n",
      "\n",
      "    1. For all packages listed above, run the following command to remove all\n",
      "       existing CuPy installations:\n",
      "\n",
      "         $ pip uninstall <package_name>\n",
      "\n",
      "      If you previously installed CuPy via conda, also run the following:\n",
      "\n",
      "         $ conda uninstall cupy\n",
      "\n",
      "    2. Install the appropriate CuPy package.\n",
      "       Refer to the Installation Guide for detailed instructions.\n",
      "\n",
      "         https://docs.cupy.dev/en/stable/install.html\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "  warnings.warn(f'''\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' # \"cuda|cpu\"\n",
    "\n",
    "if device == 'cuda':\n",
    "    import cupy as cp\n",
    "else:\n",
    "    cp = np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d233247-1378-4de9-b154-cf65b8b193e5",
   "metadata": {},
   "source": [
    "### Spacy preprocessing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba9d75d2-26e8-4e76-be4b-cc76db0b6af1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/chris/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from tqdm import tqdm\n",
    "from typing import List\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "sw_nltk = stopwords.words('english')\n",
    "\n",
    "\n",
    "email_re = r\"\"\"(?:[a-z0-9!#$%&'*+/=?^_`{|}~-]+(?:\\.[a-z0-9!#$%&'*+/=?^_`{|}~-]+)\n",
    "    *|\"(?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21\\x23-\\x5b\\x5d-\\x7f]\n",
    "    |\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])*\")@(?:(?:[a-z0-9](?:[a-z0-9-]*[a-z0-9])?\\.)+[a-z0-9]\n",
    "    (?:[a-z0-9-]*[a-z0-9])?|\\[(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.){3}\n",
    "    (?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?|[a-z0-9-]*[a-z0-9]:\n",
    "    (?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21-\\x5a\\x53-\\x7f]|\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])+)\\])\"\"\"\n",
    "\n",
    "replace = [\n",
    "    (\"<[^>]*>\", \" \"),\n",
    "    (email_re, \" \"),                           # Matches emails\n",
    "    (r\"(?<=\\d),(?=\\d)\", \"\"),                   # Remove commas in numbers\n",
    "    (r\"[*\\^\\.$&@<>,\\-/+{|}=?#:;'\\\"\\[\\]]\", \"\"), # Punctuation and other junk\n",
    "    (r\"[\\n\\t\\r]\", \" \"),                        # Removes newlines, tabs, creturn\n",
    "    (r\"[^\\x00-\\x7F]+\", \"\"),                    # Removes non-ascii chars\n",
    "    (r\"\\\\+\", \" \"),                             # Removes double-backslashs\n",
    "    (r\"\\s+n\\s+\", \" \"),                         # 'n' leftover from \\\\n\n",
    "    (r\"\\s+\", \" \"),                             # Strips extra whitespace\n",
    "        (r\"\\d+\", \"DIGIT\")                      # Map digits to special token <numbr>\n",
    "]\n",
    "\n",
    "\n",
    "def transform(doc: str):\n",
    "    for repl in replace:\n",
    "        doc = re.sub(repl[0], repl[1], doc.lower())\n",
    "    doc = \" \".join([w for w in doc.split(\" \") if not w in sw_nltk])\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe03199f-15c8-434c-a5f8-03863a8183fc",
   "metadata": {},
   "source": [
    "### Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0617ac12-e428-4960-9d59-cf70a25aaf37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " J.R. Tolkien is a gaff-topsail schooner of Netherlands registry used for passenger cruises on the Baltic Sea and elsewhere in European waters.Originally named Dierkow the vessel was built in 1964 as a seagoing diesel-electric tug at the Edgar-André-Werft in Magdeburg East Germany.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50000 [00:00<?, ?it/s]<ipython-input-4-e3b85edb49ad>:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.content[i] = transform(content)\n",
      "100%|██████████| 50000/50000 [00:10<00:00, 4880.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " jr tolkien gafftopsail schooner netherlands registry used passenger cruises baltic sea elsewhere european watersoriginally named dierkow vessel built DIGIT seagoing dieselelectric tug edgarandrwerft magdeburg east germany\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "print(df.content[300])\n",
    "\n",
    "with tqdm(total=M) as bar:\n",
    "    for i, content in enumerate(df.content.tolist()):\n",
    "        df.content[i] = transform(content)\n",
    "        bar.update(1)\n",
    "\n",
    "print(df.content[300])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eba26ee-1be0-43b1-b2e1-6766316409cb",
   "metadata": {},
   "source": [
    "### Featurizer\n",
    "\n",
    "Below we will build a BOW featurizer, and use it as both a CBOW featurizer and as an embedding lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f68c0fd-d26e-4c29-8ea1-19f5f0bcdb94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chris/anaconda3/envs/ml-base/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "24077"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "vocab_size = len(set(\" \".join(df.content.tolist()).split(\" \")))\n",
    "print(vocab_size)\n",
    "featurizer = CountVectorizer(max_features=vocab_size, min_df=4, stop_words=None)\n",
    "featurizer.fit(df.content.tolist());\n",
    "featurizer.get_idx = {word: idx for idx, word in enumerate(featurizer.get_feature_names())}\n",
    "featurizer.get_word = {idx: word for idx, word in enumerate(featurizer.get_feature_names())}\n",
    "\n",
    "N = len(featurizer.get_idx)\n",
    "N"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df714e2-fade-4db8-b994-dc2c35f6e8dd",
   "metadata": {},
   "source": [
    "### Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b89e898c-c13b-4f89-8ce0-2eb4816f57b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(dataframe):\n",
    "    # Labels\n",
    "    Y = dataframe.label.to_numpy()\n",
    "\n",
    "    # One-hot features (just word index pointers)\n",
    "    X = [[featurizer.get_idx[word] for word in doc.split(\" \") if featurizer.get_idx.get(word) is not None] \n",
    "          for doc in dataframe.content]\n",
    "\n",
    "    # BOW features\n",
    "    X_bow = np.zeros(shape=[len(dataframe), N], dtype=int)\n",
    "    for i, seq in enumerate(X):\n",
    "        for idx in seq:\n",
    "            X_bow[i, idx] += 1\n",
    "    return X_bow, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b53657",
   "metadata": {},
   "source": [
    "### Data spit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a567466b",
   "metadata": {},
   "outputs": [],
   "source": [
    "M_tr = int(0.8 * M)\n",
    "M_cv = int(0.1 * M)\n",
    "M_te = M - M_tr - M_cv\n",
    "\n",
    "X_tr, Y_tr = get_features(df[:M_tr])\n",
    "X_cv, Y_cv = get_features(df[M_tr: M_tr + M_cv])\n",
    "X_te, Y_te = get_features(df[-M_te:])\n",
    "\n",
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0977cd1-e36e-45fb-ac87-04c26fd18352",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['also',\n",
       " 'census',\n",
       " 'chahardangeh',\n",
       " 'county',\n",
       " 'district',\n",
       " 'families',\n",
       " 'gol',\n",
       " 'iran',\n",
       " 'known',\n",
       " 'population',\n",
       " 'province',\n",
       " 'rural',\n",
       " 'shahrake',\n",
       " 'tehran',\n",
       " 'village']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[featurizer.get_word[idx] for idx, c in enumerate(X_tr[0]) if c]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ab7233-c61c-438e-bc36-bcf3a50f55a1",
   "metadata": {},
   "source": [
    "## Neural network as a graph\n",
    "\n",
    "Deep learning packages implement neural networks as a graph, where each node represents a layer within the network, and each edge represents a transformation that maps one layer onto the another. The two most important components of any deep learning library are (i) efficient implementations of the various transformations used in deep learning (graph edges) that are optimized for various hardware (x86, ARM, NVIDIA GPUS, TPUs etc..), and (ii) automatic differentiation (autograd) which enables users to use the package constructs (linear transformations, activations, loss functions, optimizers etc.) without ever having to implement (or even think about) backpropogation. Below is a (very rudimentary and fairly inefficient) implementation of the basic computational units that are needed to implement a feed forward network.\n",
    "\n",
    "*Note: There are two numpy multiplication operations being used here: 1) `@` for matrix multiplication, and 2) `*` for the element-wise (aka Hadamard, aka Schur) product*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4beefbf-5a60-476f-88be-79cfe34236fa",
   "metadata": {},
   "source": [
    "### Cross entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f2a3aee-913b-41f0-9d75-3574ba307484",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(P, y):\n",
    "    ce = cp.sum(-cp.asarray(y) * cp.log(P), axis=1)\n",
    "    return cp.mean(ce)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb40c73b-9d70-4e29-95de-a93e458f6888",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1890429-529e-4333-b825-70d6dbe04acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(Y_hat, Y):\n",
    "    acc = np.equal(Y_hat, Y).mean()\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879681c5-d4d5-4223-8b6d-9d425066d557",
   "metadata": {},
   "source": [
    "### Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bfb1f2e8-c49e-4955-9a36-a8c6275b06b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(Z):\n",
    "    Z = cp.exp(Z - cp.max(Z, axis=1, keepdims=True))\n",
    "    partition = cp.sum(Z, axis=1, keepdims=True)\n",
    "    return Z / partition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f424c821-7d6c-4e42-99e4-292f387fb162",
   "metadata": {},
   "source": [
    "### Weight initializer\n",
    "\n",
    "Weight initialization is crucially important in deep learning, if not done properly your network will just diverge after the first gradient update. There are many options here, but it primarily boils down to what distribution do you want your intial weight values to be drawn from. A great choice is *Xavier* initialization, named after it's author Xavier Glorot, which draws weights from a normal distribution $W_{ij} \\sim N(\\mu=0, \\sigma^2=\\frac{1}{n})$, where $W \\in \\mathbb{R}^{n \\times \\cdot}$. For large weight matrices, this is a means to control the variance of the output, $z$, at each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af109dfe-af27-44d9-9a23-8000a7d6f426",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_init(*dims):\n",
    "    \"\"\" Xavier initialization \"\"\"\n",
    "    return (1 / cp.sqrt(dims[0])) * cp.random.randn(*dims)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7beff7c6-a128-4d83-ad46-e8b426f591a8",
   "metadata": {},
   "source": [
    "### Dropout module\n",
    "\n",
    "Dropout is one of the most important regularization techniques in deep learning. In each activation layer during training, a random subset (chosen with probability `prob`) of the activations are masked out (zeroed). On an intuitive and somewhat handwavy level, you can think of this as a way to prevent the network over relying on on a small subset of the pathways through the network, and helps force all of the nodes, and therefore weights, to be utilized (quasi) equally. By doing this, we are scaling the values of the output layer; in order to keep the distribution over each layer consistent during the training and test phases, this means we must apply a scaling factor either during test time ($\\frac{1}{prob}$, traditional dropout), or during training ($\\frac{1}{1-prob}$, inverted dropout). Inverted dropout is the preferred method as it incurs no runtime cost during inference, while only adding marginal runtime overhead during training (remember, the vast majority of the computation during training is from backpropogation). This is an implementation of inverted dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e55c06b5-8bda-4cd2-a23f-5787f873a7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_dropout(a: cp.ndarray, prob: float):\n",
    "    mask = cp.random.choice([0.0, 1.0], size=a.shape, p=[prob, 1 - prob])\n",
    "    a *= mask / (1 - prob)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3551ab-7aed-4d6c-b0a1-b4c411ea0a46",
   "metadata": {},
   "source": [
    "### Graph node\n",
    "\n",
    "The doubly linked list is a natural data structure for implementing networks. Below we make a class called `Node` which will serve as the base class for the various layer types in our network. In this implementation, each `Node` will contain three things: (i) memory to store a layer (`Node.out`), (ii) a method to map the parent node's output layer to its output (`Node.out = Node.forward(Node.last.out)`), (iii) a method to backpropogate gradients from its child node to its parent node (`Node.error = Node.backward(Node.next.error)`), and (iv) a set of methods to perform gradient updates to the parameters that it owns (`Node.update()`, `Node.zero_grad()`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67a377e7-7c63-4598-beb0-495c4371e7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    \n",
    "    \"\"\" \n",
    "    Node in computational graph (doubly linked list)\n",
    "    Note: not to be confused with a node in a network layer\n",
    "    \n",
    "    Attributes:\n",
    "    last: Node        # Parent\n",
    "    next: Node        # Child\n",
    "    out: np.ndarray   # Output layer\n",
    "    error: np.ndarray # dCE/d`out`\n",
    "    dim: int          # layer dimension\n",
    "    lr: float         # learning rate\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, last, dim: int, lr: float = None):\n",
    "        self.last = last\n",
    "        if self.last:\n",
    "            self.last.next = self\n",
    "        self.next = None\n",
    "        self.out = None\n",
    "        self.error = None\n",
    "        self.dim = dim\n",
    "        self.lr = lr\n",
    "        \n",
    "    def forward(self, dropout: bool = False):\n",
    "        pass\n",
    "    \n",
    "    def backward(self):\n",
    "        pass\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        pass\n",
    "    \n",
    "    def update(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36554d1-7855-48aa-ba81-24a3a0567ff4",
   "metadata": {},
   "source": [
    "### Input embedding node\n",
    "\n",
    "This is an embedding lookup, which will be the first layer in our network. The forward method accepts a batch of BOW features and computes their inner-product with an embedding lookup table. This is an implementation of the continuous *bag-of-features*, in which we we simply sum the embedding for each word in the the BOW input, weighted by their frequency counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "126c801f-81e9-490d-b898-747b73baaea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputEmbedding(Node):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, lr, wt_decay):\n",
    "        super().__init__(None, embedding_dim, lr)\n",
    "        self.W = weight_init(embedding_dim, vocab_size)\n",
    "        self.b = weight_init(1, self.dim)\n",
    "        self.wt_decay = wt_decay\n",
    "        self.zero_grad()\n",
    "    \n",
    "    def forward(self, X, dropout):\n",
    "        self.input = cp.asarray(X)\n",
    "        self.out = self.input @ self.W.T + self.b\n",
    "        if self.next:\n",
    "            self.next.forward(dropout)\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        self.W_grad = np.zeros_like(self.W)\n",
    "        self.b_grad = np.zeros_like(self.b)\n",
    "    \n",
    "    def update(self):\n",
    "        n = self.out.shape[0]\n",
    "        l1_grad = self.wt_decay * np.sign(self.W)\n",
    "        l2_grad = self.wt_decay * self.W\n",
    "        self.W -= self.lr * ((1 / n) * self.W_grad + l1_grad + l2_grad)\n",
    "        self.b -= self.lr * (1 / n) * self.b_grad\n",
    "    \n",
    "    def backward(self, error=None):\n",
    "        if error is None:\n",
    "            error = self.next.error\n",
    "        dW = self.input\n",
    "        db = 1\n",
    "        self.W_grad += error.T @ dW\n",
    "        self.b_grad += cp.sum(error, axis=0) * db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4032844a-d090-4ece-96d7-c7f2c0d22a18",
   "metadata": {},
   "source": [
    "### Leaky relu activation\n",
    "\n",
    "The leaky relu (rectified linear unit) is a popular activation function (non-linearity).\n",
    "\n",
    "$LReLU(z) = \\max(z, \\alpha z) \\quad \\text{where} \\quad \\alpha \\in (0, 1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ac580b06-9dc9-4539-a381-7d7dfd00fb75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LReLU(Node):\n",
    "    \n",
    "    def __init__(self, last, alpha=0.001, dropout=0.5):\n",
    "        super().__init__(last, last.dim)\n",
    "        self.alpha = alpha\n",
    "        self.dropout = dropout\n",
    "    \n",
    "    def forward(self, dropout):\n",
    "        self.out = cp.where(self.last.out >= 0, \n",
    "                            self.last.out, \n",
    "                            self.alpha * self.last.out)\n",
    "        if dropout:\n",
    "            self.out = apply_dropout(self.out, prob=self.dropout)\n",
    "        self.next.forward(dropout)\n",
    "        \n",
    "    def update(self):\n",
    "        self.last.update()\n",
    "        \n",
    "    def zero_grad(self):\n",
    "        self.last.zero_grad()\n",
    "        \n",
    "    def backward(self):\n",
    "        da = cp.where(self.last.out >= 0, 1, self.alpha)\n",
    "        self.error = self.next.error * da\n",
    "        self.last.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f68b62d-35c9-49b0-9446-f2dda8ebcaef",
   "metadata": {},
   "source": [
    "### Note on the *forward* and *backward* pass\n",
    "\n",
    "In the above cell you'll note that the `.forward()` method contains a call to `Node.next.forward()`; this is what allows data to flow from input layer to output layer. Also notice that the `.backward()`, `.update()`, and `.zero_grad()` methods all contain calls to either/both its child (`Node.next`) or parent (`Node.last`) along with code that computes gradents and updates parameter weights; this is very basic implementation of *autograd*. These features allow us to simply call `head_node.forward(some_input)` to compute the *forward pass*, and `tail_node.backward()` to compute the *backward pass* in the training loop at the bottom of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc122a96-6c52-44df-b73d-9d31e35741d4",
   "metadata": {},
   "source": [
    "### Linear node\n",
    "\n",
    "This is the *linear* unit, sometimes referred to as a `dense` layer. It's just a batched implementation of $XW^T + b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7ad8809b-6c92-4013-b880-fa69fdcbf663",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Node):\n",
    "    \n",
    "    def __init__(self, last, dim: int, lr: float, wt_decay: float, has_bias: bool = True):\n",
    "        super().__init__(last, dim, lr)\n",
    "        self.has_bias = has_bias\n",
    "        self.W = weight_init(self.dim, self.last.dim)\n",
    "        if has_bias:        \n",
    "            self.b = weight_init(1, self.dim)\n",
    "        self.wt_decay = wt_decay\n",
    "        self.zero_grad()\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        self.W_grad = cp.zeros_like(self.W)\n",
    "        if self.has_bias:\n",
    "            self.b_grad = cp.zeros_like(self.b)\n",
    "        self.error = None\n",
    "        self.last.zero_grad()\n",
    "    \n",
    "    def update(self):\n",
    "        n = self.out.shape[0]\n",
    "        l2_grad = self.wt_decay * self.W\n",
    "        self.W -= self.lr * ((1 / n) * self.W_grad + l2_grad)\n",
    "        if self.has_bias:\n",
    "            self.b -= self.lr * (1 / n) * self.b_grad\n",
    "        self.last.update()\n",
    "    \n",
    "    def forward(self, dropout):\n",
    "        self.out = self.last.out @ self.W.T\n",
    "        if self.has_bias:\n",
    "            self.out += self.b\n",
    "        if self.next:\n",
    "            self.next.forward(dropout=dropout)\n",
    "        \n",
    "    def backward(self, error=None):\n",
    "        if error is None:\n",
    "            error = self.next.error\n",
    "        dW = self.last.out\n",
    "        self.W_grad += error.T @ dW\n",
    "        if self.has_bias:\n",
    "            db = 1\n",
    "            self.b_grad += np.sum(error, axis=0) * db\n",
    "        self.error = error @ self.W\n",
    "        self.last.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9b3be6-73eb-437e-86c3-1bf157b664d4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Network wrapper\n",
    "\n",
    "This is a wrapper around our doubly linked list, it allows us to define one object for interacting with the network rather than having to keep track of the *head* and *tail* nodes of our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "55a12444-15de-49d9-a87d-d63b57bf096c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    \n",
    "    input: Node\n",
    "    last: Node\n",
    "    probs: np.ndarray\n",
    "    pred: np.ndarray\n",
    "    \n",
    "    def set_lr(self, lr):\n",
    "        node = self.input\n",
    "        while node is not None:\n",
    "            node.lr = lr\n",
    "            node = node.next\n",
    "            \n",
    "    @property\n",
    "    def lr(self):\n",
    "        return self.last.lr\n",
    "        \n",
    "    @property\n",
    "    def out(self):\n",
    "        return self.last.out\n",
    "    \n",
    "    def forward(self, x, dropout=False):\n",
    "        self.input.forward(x, dropout=dropout)\n",
    "        self.probs = softmax(self.out)\n",
    "        self.pred = cp.argmax(self.probs, axis=1).get()\n",
    "    \n",
    "    def backward(self, Y):\n",
    "        loss = self.probs - cp.asarray(Y)\n",
    "        self.last.backward(loss)\n",
    "        \n",
    "    def update(self):\n",
    "        self.last.update()\n",
    "        \n",
    "    def zero_grad(self):\n",
    "        self.last.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c1c69e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANN(Network):\n",
    "    def __init__(self, vocab_size, layer_size, output_dim, num_layers, lr, wt_decay, alpha, dropout):\n",
    "        super().__init__()\n",
    "        self.input = InputEmbedding(vocab_size, layer_size, lr, wt_decay)\n",
    "        self.last = LReLU(self.input, alpha=alpha, dropout=dropout)\n",
    "        for i in range(num_layers - 1):\n",
    "            z = Linear(self.last, layer_size, lr, wt_decay)\n",
    "            self.last = LReLU(z, dropout=dropout)\n",
    "        self.last = Linear(self.last, output_dim, lr, wt_decay, has_bias=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502ac479",
   "metadata": {},
   "source": [
    "### Training hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f3478014-45bf-427f-84b1-e1b39b2c9c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.001\n",
    "lr_init = 0.008\n",
    "lr_min = 0.0001\n",
    "lr_decay = 0.975\n",
    "wt_decay = 1e-4\n",
    "dropout = 0.1\n",
    "layer_size = 400\n",
    "layers = 2\n",
    "batch_size = 16\n",
    "batch_size_inference = 512\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afcf432-6efd-4174-87b2-8f8a9c5679c5",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "83077fe8-f879-40cc-8e0b-f92f6ca4ac20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 50, lr: 0.00231, CE (train/val): 0.0338 / 0.1571, Acc (train/val): 0.9913 / 0.9590: 100%|██████████| 125000/125000 [33:18<00:00, 62.54it/s]  \n"
     ]
    }
   ],
   "source": [
    "net = ANN(N, layer_size, K, layers, lr=lr_init, wt_decay=wt_decay, alpha=alpha, dropout=dropout)\n",
    "\n",
    "one_hot_labels = np.eye(K)\n",
    "\n",
    "shuffle_idx = np.arange(M_tr)\n",
    "\n",
    "with tqdm(total=epochs * (M_tr // batch_size)) as bar:    \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        # Randomize data ordering\n",
    "        np.random.shuffle(shuffle_idx)\n",
    "        X_shfd = X_tr[shuffle_idx]\n",
    "        Y_shfd = Y_tr[shuffle_idx]\n",
    "        \n",
    "        for i in range(0, M_tr, batch_size):\n",
    "            \n",
    "            # Get batch of x,y pairs\n",
    "            x = X_shfd[i: i + batch_size]\n",
    "            y = Y_shfd[i: i + batch_size]\n",
    "            \n",
    "            # Skip empty slices\n",
    "            if not x.shape[0]:\n",
    "                continue\n",
    "            \n",
    "            # Forward pass\n",
    "            net.forward(x, dropout=True)\n",
    "            \n",
    "            # Backward pass\n",
    "            # Note: From lecture 05, remember that the derivative of the\n",
    "            # cross entropy loss w.r.t. the input (z) to the softmax P(z)\n",
    "            # is dCE/dz = P(z) - y.\n",
    "            net.backward(one_hot_labels[y])\n",
    "            \n",
    "            # Apply gradient updates\n",
    "            net.update()\n",
    "            \n",
    "            # Zero the stored gradients\n",
    "            net.zero_grad()\n",
    "            \n",
    "            bar.update(1)\n",
    "        \n",
    "        # Evaluate performance on training set w/o dropout\n",
    "        tr_results = []\n",
    "        for i in range(0, M_tr, batch_size_inference):\n",
    "            x = X_tr[i: i + batch_size_inference]\n",
    "            y = Y_tr[i: i + batch_size_inference]\n",
    "            net.forward(x)\n",
    "            ce_tr = cross_entropy(net.probs, one_hot_labels[y])\n",
    "            acc_tr = compute_accuracy(net.pred, y)\n",
    "            tr_results.append([x.shape[0] / M_tr, ce_tr, acc_tr])\n",
    "        \n",
    "        ce_tr = sum([x[0] * x[1] for x in tr_results])\n",
    "        acc_tr = sum([x[0] * x[2] for x in tr_results])\n",
    "        \n",
    "        net.forward(X_cv)\n",
    "        ce_cv = cross_entropy(net.probs, one_hot_labels[Y_cv])\n",
    "        acc_cv = compute_accuracy(net.pred, Y_cv)\n",
    "\n",
    "        bar.set_description(\"Epoch: %d, lr: %.5f, CE (train/val): %.4f / %.4f, Acc (train/val): %.4f / %.4f\" \n",
    "                            % (epoch + 1, net.lr, ce_tr, ce_cv, acc_tr, acc_cv))\n",
    "\n",
    "        # Learning rate annealing\n",
    "        net.set_lr(max(lr_min, lr_decay * net.lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b7e222",
   "metadata": {},
   "source": [
    "### Compute test performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5dacf06a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test CE: 0.1724, Test Acccuracy: 0.9548\n"
     ]
    }
   ],
   "source": [
    "net.forward(X_te)\n",
    "ce_te = cross_entropy(net.probs, one_hot_labels[Y_te])\n",
    "acc_te = compute_accuracy(net.pred, Y_te)\n",
    "print(f\"Test CE: {ce_te:.4}, Test Acccuracy: {acc_te}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "aaf90f38dd02d397fb672f3eedea9f67f57967a2a80a762c925841e1b54a5d86"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
